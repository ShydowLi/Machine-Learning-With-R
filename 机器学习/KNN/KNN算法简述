----懒惰学习——近邻分类

    一、懒惰学习：
        基于近邻分类的分类算法都是被认为时懒惰学习，在技术上没有抽象化的步骤。抽象过程和一般化过程都被跳过了。
        懒惰学习并不是真正的在学习什么，它只是一字不差的存储训练数据，训练阶段进行的很快，但在进行预测的过程中会变得比较慢。同时又被称为基于实例的学习或者机械学习。
        由于基于实例的学习算法并不会建立一个模型，所以该方法归类为非参数学习方法，即没有需要学习的数据参数。所以非参数方法限制了我们理解分类器如何使用数据的能力，另一方面，它并不是将数据拟合为一个预先设定的可能的偏差函数形式
        
        
    二、近邻分类基本原理
        相似的东西可能具有相似的属性
      **定义分类器的关键概念，通过距离测量案例的相似性，使用KNN近邻分类器
        所谓的近邻分类是把未标记的案例归类为与他们最相似的带有标记的案例所在类
        
        
    三、KNN算法
        1、优点：简单有效、数据分布没有要求、训练阶段快
           缺点：没有模型，没有函数调优权衡、需要选择合适的K值、分类阶段慢、名义和有序变量需要额外处理
           
        2、概念：使用关于一个案例的k个近邻的信息来分类无标记的例子，knn算法将特征处理为一个多维空间的坐标
        
        3、基本原理：
           通过距离度量相似性：
              寻求一个距离函数或者一个用来度量两个案例之间相似性的公式
              KNN采用传统上的欧式距离（最短路径），当然也可以采用如曼哈顿距离、余弦值等
              欧式距离：     
                   √((x2-x1)^2+(y2-y1)^2 )
              
           选择一个合适的K：
              确定用于KNN算法的邻近数量k值将决定把模型推广到未来数据时模型的好坏
              过度拟合与欠拟合训练数据集之间的平衡问题称为"偏差--方差的权衡"
              K值越小：一旦有噪声的成分存在，将对预测结果产生较大的影响
              K值越大：近似误差偏大，较远的训练实例会对预测结果有影响，而且容易忽视不易察觉但重要的模式
              K值应该尽量选取奇数，保证计算结果产生较多类别
              
              K值的选取：
                 从k等于训练集案例数量的平方根开始
                 测试k值，计算k去不同值时的误差
                 
           准备knn使用的数据：
              对自变量特征进行标准化：最大-最小标准化 z分数标准化（scale）
              对因变量名义变量进行哑变量编码
              
              
              
          
              
              
              
            
              
              
              
              
          
        
            
      

