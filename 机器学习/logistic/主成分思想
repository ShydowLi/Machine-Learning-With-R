数据降维：从数据中提取公共部分，进行分析和处理
方法：主成分分析、因子分析、典型相关分析
      主成分分析是一种通过降维技术把多个变量化成少数几个主成分的方法，
  这些主成分能够反映原始变量的大部分信息，他们通常表示为原始变量的线性组合。

R中运用到的函数：  
#R中作为主成分分析最主要的函数是princomp()函数
#princomp()主成分分析   可以从相关阵或者从协方差阵做主成分分析
#summary()提取主成分信息 
#loadings()显示主成分分析或因子分析中载荷的内容
#predict()预测主成分的值 
#screeplot()画出主成分的碎石图 
#biplot()画出数据关于主成分的散点图和原坐标在主成分下的方向
#engin（cor（））计算特征值，$values 特征值 $vectors特征向量
#最终得分： 特征值*得分矩阵

案例：采用R中自带的数据集 swiss 数据集，为瑞士各个地区的教育、商务等发展情况，利用主成分分析可以看出
      各地区的发展情况，以及swiss的发展水平。
	  
	  attach(swiss)
	  mydata<-scale(swiss)
	  cor(mydata)
	  pri<-princomp(mydata,cor=true)  #这里cor=true表示用相关系数做主成分，false表示用协方差cov
	  summary(pri,loadings=true)   #输出载荷
	  screeplot(pri,type='line')   #散石图确定主成分个数
	  pre<-predict(pri) #用predict()计算得分  也可以直接pri$scores 查看得分 
	  summary(pre)  #各项得分
	  y<-eigen(cor(pdat))  #计算特征值  变量为相关系数矩阵 y$values特征值  y$vectors特征向量
      score<-(y$values[1]*pre[,1]+y$values[2]*pre[,2]+y$values[3]*pre[,3]+y$values[4]*pre[4])/sum(y$values)
      #计算每个城市的最终得分  用特征值*pre中的每一列

PCA的思想：
    PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据(x(1),x(2),...,x(m))(x(1),x(2),...,x(m))。我们希望将这m个数据的维度从n维降到n'维，希望这m个n'维的数据集尽可能的代表原始数据集。
    
PCA算法总结：
    作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如第六节的为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

　　　　PCA算法的主要优点有：

　　　　1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

　　　　2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

　　　　3）计算方法简单，主要运算是特征值分解，易于实现。

　　　　PCA算法的主要缺点有：

　　　　1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

　　　　2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

 
      	  
	  
