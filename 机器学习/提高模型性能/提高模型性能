==-----提高模型的性能--------------------------------------

    1、如何通过系统化的寻找训练条件的最佳集合来自动调整模型的性能
    2、将多个模型组合起来从而处理更有难度的学习任务的方法
    3、如何使用决策树的变种
    
==-------------------------------------------------------------------

--、使用CARET包进行自动参数调整
    
    自动参数调整需要考虑需要考虑以下三类问题：
        1、对数据应该训练那种机器学习模型
        2、哪些模型参数可以调整，为了找到最优设置对他们进行调整强度有多大
        3、使用何种评价标准来评估模型从而找到最优的模型候选者

    caret包中对p个参数中的每一个参数最多搜索3个可能值，这一位着最对优3^p个候选模型被测试。
    
    
    自选模型的三个步骤：
        1、搜索候选模型的集合，该集合由所有参数组合的矩阵或者网格的形式构成  （3^p个参数调整）
        2、从众多模型中识别出最好的模型（使用k折cv或者使用度量准确率的模型性能统计量筛选模型）
        
    
    caret包中的train()函数和selectfunction()函数联合使用，自动创建简单模型和定制调整模型
    
    
==----------------------------------------------------------------------------------------------------

--、使用元学习来提高模型的性能

    1、元学习是指将多个模型合并成一个更强的组，一种通过自适应、自修改学习方式的高度复杂算法。
    
    集成学习：
        如何选择或者构造那些较弱的学习模型
        如何将这些较弱的学习模型的预测结果组合起来形成最终的预测
        
    模式：
        训练数据--分配数据--model1、model2、model3--组合函数--组合模型
        
        分配函数：决定每个模型接收多少训练数据集，每个模型接收的是完整的数据集还是某个抽样样本，模型是接受一个特     征还是不同特征的子集。
        组合函数：对预测中的不一致进行调解（采用投票加权来决定最终的预测）
        
        优点：
            1、对未来问题更好的适应
            2、可以提升大量数据或少量数据的性能
            3、将不同领域数据合成的能力
            4、对于困难学习任务更细致的理解
            
            
    2、bagging（自助汇聚法）
        对原始训练数据使用自助抽样法的方式产生多个训练数据集，使用单一的机器学习算法产生多个模型，然后使用投票或     者平均的方法组合预测值。一般和决策树一起使用。
        
        使用ipred添加包中的bagging()函数，具体用法为：bagging(model,data,nbagg)  nbagg参数用来调整数量
        
        
        
    3、boosting
        增加弱学习器的性能来获得强学习器的性能。同样也是使用在不同的重抽样数据中训练模型的集成，并通过投票来决定     最终的预测值。
        
        与bagging的不同：
            1、重抽样数据集的构建是专门用来产生互补的模型
            2、选票并不是同等重要，需要根据之前的表现进行加权，也就意味着性能好的模型对预测有更大的影响
            
        boosting构建集成学习中的模型是互补的，所以通过简单的为组添加其他学习器的方式将性能提升到任意阈值是可能的     前提是每一个分类器的性能优于随机分类。
        
        原理：分类器迭代，产生弱分类器来迭代地学习训练集中很大比例难以分类的样本，对经常分错类的样本进行更多的关     注（给予更大的权重），
        



        
        