==----模型性能评价---------------------------------------

    目标：
        为什么预测准确率不足以度量性能，以及替代的度量性能的方法
        用来确保性能度量方法可以合理的反映模型对于未知情形的预测能力的方法
        怎样将这些有用的度量应用于前面的学习案例
        
    1、度量分类方法的性能：
        目的：最好的度量方式要看其是否能成功的实现预期目标，拥有能够度量实用性而不是原始准确率的模型评价方法是至         关重要的。评价一个分类器性能的目的在于能更好的了解其对未来情形的预测能力
        
    性能度量指标：  
      
        混淆矩阵： 阳性：我们感兴趣的类别，  阴形： 我们不感兴趣的类别
            真阳性（TP）：正确的分类为感兴趣的类别
            真阴性（TN）：正确的分类为不敢兴趣的类别
            假阳性（FP）：错误的分类为感兴趣的类别
            假阴性（FN）：错误的分类为不感兴趣的类别
            
            准确率=（TP+TN）/(TP+TN+FP+FN)
            
            
        caret包中的confusionmatrix()函数：
            1、Kappa统计量：通过完全因为巧合而被预测正确的概率来对准确率进行调整（当存在数据类别不均衡是很重要）
               Kappa统计量仅对正确判断的分类器给予加分，而不是采取简单的猜测策略的分类器
               Kappa统计量的范围是0--1，越接近1说明与真实值越一致
               
               很差一致性：0--0.2
               尚可一致性：0.2--0.4
               中等一致性：0.4--0.6
               不错一致性：0.6--0.8
               很好一致性：0.8--1
               当然这些都是主观的，具体的问题具体分析
               
               k=Pr(a)-Pr(e)/1-Pr(e)      Pr(a):分类器和真实值之间的真实一致性比例 Pr(e):分类器和真实值之间的期望            一致性的比例。
                 Pr(a)；实际就是准确率   Pr(e)：p（实际为1）*p（预测为1）+p（实际为2）*p（预测为2）
                 
                 
            2、灵敏度和特异性(两者都是在实际值中考虑)
               灵敏度（真阳性率）：度量阳性样本被正确分类的的比例
                              灵敏度=TP/TP+FN
                              
               特异性（真阴性率）：度量阴性样本被正确分类的比例
                              特异性=TN/TN+FP
               
                              
            3、精确度和找回率
                        
                        精确度=tp/tp+fp
                        召回率=tp/tp+fn
                        
            4、F度量（F计分）
                        F度量=2*精确度*召回率/精确度+召回率
                        
            
            5、性能权衡可视化(ROCR):ROC曲线
            
library(pROC)
pre1<-predict(model,mydata,type = 'response')
summary(pre1)
modelroc<-roc(mydata$y,pre1)
plot(modelroc,print.auc=TRUE,auc.polygon=TRUE,grid=c(0.1,0.2),grid.col=c("green","red"),max.auc.polygon=TRUE,auc.polygon.col="skyblue",print.thres=TRUE)




==--------------------------------------------------------------------------------------------------------------

2、评估模型未来的性能
   可能存在这样一种情况，我们的数据量很小，不能够划分训练集和测试集情况下，这时就需要用一些别的评估模型进行对其未见过数据的性能
   
   保持法：将数据划分为训练集和测试集的过程，但是在进行构建模型的时候，我们可能会构建多个模型，再将测试集带入测试，选取最好结果的那个模型，但是在这一个过程中，我们重复使用了测试集，所以我们只是基于当前测试集而选取了最好的模型，那么它对未来数据的评估将没有多大的作用！
   
   为了解决上面的问题，所以我们应该再对数据在进行划分，在训练集和测试集之后，再添加一个验证集。验证集对模型进行迭代和改善，测试数据集只使用一次。
   
   
==---------------------------------------------------------------------------------------------------------------

3、随机抽样的弊端
   在进行随机抽样的过程中，每个划分包含某些类别的数量可能会过大或者过小。在某种特定的情况下，某些类别比例很小时，就可能导致训练集中不包含该类数据（或者很少，导致数据不平衡）
   
   为了解决以上的问题，我们便使用分层抽样方法，确保在数据集中每一个类别的比例与总体数据集中的比例近似相等。
  
   使用caret包中提供了creatDataPartition()函数
   使用方法：
            in_train<-creatDataPartition(credit$default,p=0.75,list=F)    credit代表类别
            train<-credit[in_train,]
            test<-credit[-in_train,]
            
   分层抽样虽然使样本的类别均匀，但是对于某些类别包含过多或者过少的困难样本，极端值，在数据量少时没有足够的划分。所以这种抽样方法过于保守。
   

==-------------------------------------------------------------------------------------------------------------

4、重复保持技术：
   保持法的特殊形式，缓解随机构建训练集问题。对多个随机保持样本的模型评估，然后用结果的均值来评价整个模块的性能
   
   1、交叉验证：
      k折交叉（k折cv）：将数据分割成k个完全分割开的部分，目前一般将k设为10.（也就是每一折包含数据的10%），机器学习模型使用剩下的90%建立模型。重复10次，输出所有折的平均性能指标。
      
      使用caret包中的creatfolds()函数来创建交叉验证的数据集：
        
          folds<-creatfolds(credit$default,k=10)     这里的folds只是数据的行数
          
      folds的形式是：folds01,fold02......folds10
      
          故：train01<-credit[-folds01,]
              test<-credit[folds,]
      像上面一样将训练集和测试集分为10组，最后分局模型结果求平均性能指标。。。。。可以使用lapply函数依次带入模型
      
      
   2、自助法验证
      上面的k折cv是样本数据分割成两个不同的部分，而在自助法中是将数据样本有放回的抽取，有些样本可能是重复的，剩下的数据同样作为测试集。
      
      0.632自助法：  错误率=0.632*错误率（测试）+0.328*（训练）
      
      
      
      
      
              
          
          
   
            
            
            
            
                        
                  
                 
               
               
                
            
            
        