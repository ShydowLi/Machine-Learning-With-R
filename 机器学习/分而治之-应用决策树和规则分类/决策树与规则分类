-------------------应用决策树和规则分类---------------------------------------
    1、决策树：利用树形结构对特征和潜在结果之间关系建立模型
            用途：信用评估模型：其中导致申请被拒绝的准则需要清楚的记录且没有偏差
                  客户流失或者客户满意度行为的市场调查将于广告公司或者管理机构共享
                  基于实验室测量、症状或者疾病进展率的医疗条件诊断
                  
    2、原理（分而治之）：决策树建立使用一种称为递归划分的探索法，即将数据集分为子集，然后反复的分解成更小的子集，直到算法                          决定数据内的子集足够均匀或者另一种停止准则已经满足。
    
    3、C5.0决策树算法：
       优点：适用于大多数分类问题的分类器；高度自动化的学习过程，可以处理数值数据，名义特征；排除不重要的特征；数据集数量              不限
       缺点：在具有大量水平特征进行划分时往往是有偏差的，很容易过度拟合或欠拟合；依赖于轴平行分割对某些关系建立模型时会               有困难；小的变化会引起大的变化
       
    4、步骤：
          1、选择最优的分割
             一个案例子集仅包含单个类的程度称为纯度，由单个类构成的任意子集都认为是纯的。
             C5.0算法在一个类集合中使用熵，具有高熵值的集合非常多样化，且提供关于可能属于这些集合的其他项的信息很少。
             对于N个类，熵值的范围为0~log2（n），在一个案例中，最小值表示样本同质，最大值表示数据尽可能的多样化
             
             熵的定义：  Entropy(S)=求和（-pilog2(pi)）
             
             信息增益=Entropy(s1)-Entropy(s2)   其中s2计算：求和wi*Ebtropy（pi）
             
          2、修剪决策树
             决策树增长的过大，将使许多决策过于具体，模型过度拟合训练数据。而修剪一颗决策树涉及减小他的大小，以使决策树           能够能够更好的推广到未知数据。
             提前停止法：当决策树达到一定数量的决策，或者当决策节点仅含有少量案例时，就停止树的增长（预剪枝）
             后剪枝决策树法：决策树生长得过大，就修剪叶节点，减少到合适的大小。
             
             
    5、综上决策树的步骤：
       1、开始把所有数据看作是一个节点
       2、遍历每一个变量的每一种分割方式，找到做好的分割点
       3、分割成两个节点N1和N2
       4、对N1和N2分别执行2-3步，直到每一个节点都足够纯，或者达到每一停止情况（没有特征可分，达到最大迭代次数）
       
       
       
       
       
       
       
==------------------------------------------------------------------------------------------------------------------------
--理解分类规
    1、分类规则代表的是逻辑if-else语句形式的知识，可用来对无标记的案例指定一个分类。无标记的案例根据前件和后件的概念来指定，而前件与后件构成了一个假设，即：如果这种事情发生，那么那种情况就会发生。前件是由特征值的特定组合构成的，当规则的条件被满足时，后件描述用来指定的分类值。
    用于：确定导致机械设备出现硬件故障的条件
          描述用于客户细分市场人群的关键特征
          发现某种事物大跌或者大涨的前提条件
          
    不同：决策树必须从上到下通过一系列决策应用，而规则是可以被阅读的命题，事实的陈述。模型结果规则学习跟简单，更直接理解
    
    2、方法：确定训练数据中覆盖一个案例子集的规则，然后再从剩余数据中分离出该子集，随着规则的增加，更多的数据子集会被分离，直到整个数据都被覆盖，不再有剩余案例。（向下钻取数据）
    
==-----------------------------------------------------------------------------------------------------------------------

一、1R算法:
    通过选择一个单一的规则来提高算法的性能，该算法的运作方式：对于每一个特征，1R基于相似的特征值对数据分组，然后对于每一个数据分组，该算法预测大多数类，规则错误率的计算基于每一个特征，犯最少错误的规则选为唯一的规则。
    
    RIPPER算法（重复增量修剪）：
        生长、修剪、优化
        生长阶段对规则添加条件，直到该规则完全划分为一个数据子集或者没有属性可用于分割。同决策树一样，利用信息增益准则确定下一个分割的属性，当增加一个规则的特异性而熵不在减小时，该规则需立即修剪。重复生长和裁剪两个阶段，直到达到停止准则，然后利用探索法进行整套规则优化。
        
        
    贪婪算法：因为决策树和规则学习都是基于想到想得的思想使用数据，它们都试图一次性分区，首相找到同质性最好的分区，接着是次好，直到都被归类。
     
     决策树与规则的不同：决策树一旦根据某一特征分割后，那么分割所创的分区将不会重新占据，而规则学习，没有被规则条件覆盖的样本可能会被重新占据。
     
     
    
    
    



       
       
             
             
             
             
       
     
     